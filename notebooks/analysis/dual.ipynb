{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39891d-fcfa-456d-8b3e-be03bf6fce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f770a7-0a5e-493b-a4b1-e41443355ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967d3d2-94b2-4dc8-9847-5e163054fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, minmax_scale\n",
    "from sklearn.metrics import root_mean_squared_error, ndcg_score\n",
    "def calc_test(true_scores, pred_scores, k=10):\n",
    "    rho, _ = stats.spearmanr(true_scores, pred_scores)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = root_mean_squared_error(true_scores, pred_scores)\n",
    "\n",
    "    # NDCG@k\n",
    "    std_tgts = minmax_scale([true_scores], (0, 5), axis=1)\n",
    "    ndcg_val = ndcg_score(std_tgts,[pred_scores], k=k)\n",
    "\n",
    "    result ={\n",
    "        'spearman': rho,\n",
    "        # 'rmse': rmse,\n",
    "        'ndcg': ndcg_val\n",
    "    }\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "def calculate_mean_similarity(latent_matrix: np.ndarray):\n",
    "\n",
    "    # 入力チェック\n",
    "    if not isinstance(latent_matrix, np.ndarray):\n",
    "        raise TypeError(\"latent_matrix must be numpy.ndarray\")\n",
    "\n",
    "    if len(latent_matrix.shape) != 2:\n",
    "        raise ValueError(\"latent_matrix must be 2-dimensional\")\n",
    "\n",
    "    N, H = latent_matrix.shape\n",
    "\n",
    "    if N < 2:\n",
    "        raise ValueError(\"Number of samples must be greater than 1\")\n",
    "\n",
    "    # 各ベクトルのノルムを計算\n",
    "    norms = np.linalg.norm(latent_matrix, axis=1, keepdims=True)\n",
    "    # ゼロ除算を防ぐ\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "\n",
    "    # 正規化された行列を計算\n",
    "    normalized_matrix = latent_matrix / norms\n",
    "\n",
    "    # コサイン類似度行列を計算\n",
    "    similarity_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "    # 対角要素を0にする（自己との類似度は除外）\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "\n",
    "    # 平均コサイン類似度を計算\n",
    "    mean_similarity = similarity_matrix.sum() / (N * (N-1))\n",
    "\n",
    "    return mean_similarity\n",
    "\n",
    "import logomaker\n",
    "def draw_logo(seqs, ax=None):\n",
    "    if isinstance(seqs, str):\n",
    "        seqs = [seqs]\n",
    "    counts_matrix = logomaker.alignment_to_matrix(seqs)\n",
    "\n",
    "    logo = logomaker.Logo(counts_matrix,\n",
    "            shade_below=.5,\n",
    "            fade_below=.5,\n",
    "            color_scheme='NajafabadiEtAl2017',\n",
    "            ax=ax\n",
    "        )\n",
    "    logo.ax.spines['right'].set_visible(False)\n",
    "    logo.ax.spines['top'].set_visible(False)\n",
    "    logo.ax.spines['bottom'].set_visible(False)\n",
    "    logo.ax.spines['left'].set_visible(False)\n",
    "    # logo.ax.set_xticks(np.arange(length))\n",
    "    logo.ax.set_yticks([])\n",
    "\n",
    "def get_ddg(path):\n",
    "    results_df = pd.read_csv(path)\n",
    "\n",
    "    ddg_scores = (results_df[results_df[\"scored_state\"]==\"ddG\"]\n",
    "                     .groupby(\"case_name\")[\"total_score\"]\n",
    "                     .min()\n",
    "                     .sort_index())\n",
    "    return ddg_scores\n",
    "\n",
    "def mutstr_to_mutseq(mutstr, wt_seq, offset=0, indel_indices=None):\n",
    "    if indel_indices is None:\n",
    "        indel2indices = {i+offset:i for i in range(len(wt_seq))}\n",
    "    else:\n",
    "        indel2indices = {v:i for i,v in enumerate(indel_indices)}\n",
    "    mutseq = list(wt_seq)\n",
    "    mutations = mutstr.split(',')\n",
    "\n",
    "    for mutation in mutations:\n",
    "        # wt, pos, mut = mutation[0], int(mutation[2:-1]) - offset, mutation[-1]\n",
    "        wt, pos, mut = mutation[0], int(indel2indices[int(mutation[2:-1])]), mutation[-1]\n",
    "        assert wt == mutseq[pos]\n",
    "        mutseq[pos] = mut\n",
    "    return ''.join(mutseq)\n",
    "\n",
    "def mutseq_to_mutstr(mutseq, wt_seq, chain, offset=0, indel_indices=None):\n",
    "    if indel_indices is None:\n",
    "        indices2indel = {i:i+offset for i in range(len(wt_seq))}\n",
    "    else:\n",
    "        indices2indel = {i:v for i,v in enumerate(indel_indices)}\n",
    "    mutations = []\n",
    "    assert len(mutseq)==len(wt_seq)\n",
    "    for i, (wt, mut) in enumerate(zip(wt_seq, mutseq)):\n",
    "        if wt != mut:\n",
    "            pos = indices2indel[i]\n",
    "            mutations.append(f\"{wt}{chain}{pos}{mut}\")\n",
    "\n",
    "    return ','.join(mutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48f699-a000-4d31-9a01-a47c667e8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pygmo import hypervolume\n",
    "\n",
    "def greedy_hypervolume_subset(points, n, ref_point):\n",
    "    selected = []\n",
    "    remaining = list(range(len(points)))\n",
    "\n",
    "    for _ in tqdm(range(n)):\n",
    "        max_hv = -float('inf')\n",
    "        best_idx = None\n",
    "\n",
    "        for idx in remaining:\n",
    "            # 現在の選択 + 候補点のHypervolume計算\n",
    "            current_points = points[selected + [idx]]\n",
    "            hv = hypervolume(current_points)\n",
    "            current_hv = hv.compute(ref_point)\n",
    "\n",
    "            if current_hv > max_hv:\n",
    "                max_hv = current_hv\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_idx is not None:\n",
    "            selected.append(best_idx)\n",
    "            remaining.remove(best_idx)\n",
    "\n",
    "    return selected, max_hv\n",
    "\n",
    "def normalize_score(score):\n",
    "    return (score-score.quantile(0.05))/(score.quantile(0.95)-score.quantile(0.05)+1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204adaf-23ec-42d7-9179-23a0219767a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_dict = {\n",
    "    \"4D5_HER2_fitness_1N8Z\":   \"SRWGGDGFYAMDY\",\n",
    "    \"5A12_Ang2_fitness_4ZFG\":  \"ARFVFFLPYAMDY\",\n",
    "    \"5A12_VEGF_fitness_4ZFF\":  \"ARFVFFLPYAMDY\",\n",
    "}\n",
    "\n",
    "exp2name={\n",
    "    \"greedy\":\"Greedy\", \"greedy_unbias\": \"Greedy(no bias)\",\n",
    "    \"greedy_unbias_offline\": \"Greedy-Offline(no bias)\", \"greedy_offline\": \"Greedy-Offline\",\n",
    "    \"ucb\": \"UCB\", \"ucb_unbias\": \"UCB(no bias)\", \"ga\": \"GA\", \"ucb_offline\": \"UCB-Offline\",\n",
    "    \"greedy_multi\": \"Greedy(Multi-Objective)\", \"ucb_multi\": \"UCB(Multi-Objective)\",\n",
    "    \"ucb_multi_offline\": \"UCB-Offline(Multi-Objective)\", \"greedy_multi_offline\": \"Greedy-Offline(Multi-Objective)\",\n",
    "    \"greedy_sum\": \"Greedy(Sum)\",\"greedy_sum_offline\": \"Greedy-Offline(Sum)\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc26d5d-6475-4f22-bf6e-40f51e98dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dual = [\"greedy\", \"greedy_multi\"]\n",
    "font_size=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0841d3-31b8-4e3b-9ff9-c5ae4a4b20c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobdf = pd.read_csv(\"jobs/job_dual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7cac9-44d1-4530-b44d-415a4a3b8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e2604-4188-440b-94d3-4208527caf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ebf0b-8911-468b-ab66-e3c7aaa2b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cycles = {}\n",
    "dfs = []\n",
    "configs = []\n",
    "for confpath in jobdf[\"CONFIG\"]:\n",
    "    with open(confpath) as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    target=data[\"data_dir\"].split(\"/\")[1]\n",
    "    model_type = data[\"data_dir\"].split(\"/\")[2]\n",
    "    exp=data[\"data_dir\"].split(\"/\")[3]\n",
    "    dfs_ = []\n",
    "    for du_target in [\"target_0\",\"target_1\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, \"..\", data[\"data_dir\"], \"9\", du_target, \"train_data\", \"training_data.csv\"))\n",
    "        df[\"target\"]=target\n",
    "        df[\"model_type\"]=model_type\n",
    "        dfs_.append(df)\n",
    "\n",
    "    df = dfs_[0].copy()\n",
    "    df[\"DMS_score_0\"] = dfs_[0][\"DMS_score\"]\n",
    "    df[\"DMS_score_1\"] = dfs_[1][\"DMS_score\"]\n",
    "    df = df.drop(\"DMS_score\",axis=1)\n",
    "    df[\"flxddg_0\"] = -dfs_[0][\"DMS_score\"]\n",
    "    df[\"flxddg_1\"] = -dfs_[1][\"DMS_score\"]\n",
    "\n",
    "    df[\"mutations\"] = df[\"mutations\"].fillna(\"\")\n",
    "    df[\"mutations_wt\"] = df[\"mutseq\"].apply(lambda x: mutseq_to_mutstr(x, h3_dict[\"4D5_HER2_fitness_1N8Z\"], \"B\", offset=0))\n",
    "    df[\"exp\"]=exp\n",
    "    score_cols = [\"flxddg_0_std\", \"flxddg_1_std\", \"ablang2_perplexity_std\"]\n",
    "    score_cols = [\"flxddg_0_std\", \"flxddg_1_std\", \"ablang2_perplexity_std\"]\n",
    "\n",
    "    # hv\n",
    "    df[\"flxddg_0_std\"] = normalize_score(df[\"flxddg_0\"])\n",
    "    df[\"flxddg_1_std\"] = normalize_score(df[\"flxddg_1\"])\n",
    "    df[\"ablang2_perplexity_std\"] = normalize_score(df[\"ablang2_perplexity\"])\n",
    "    df[\"IP_seq_std\"] = normalize_score(-df[\"IP_seq\"])\n",
    "\n",
    "    for score_col, ref_point in zip(score_cols, ref_points):\n",
    "        df[score_col]*=ref_point\n",
    "    df[\"#Mutation\"]=df[\"mutations_wt\"].apply(lambda x: len(x.split(\",\")) if x !=\"\" else 0)\n",
    "    df[\"sum_score\"]=df[score_cols].sum(axis=1)\n",
    "    df[\"sum_score_2\"]=df[score_cols[:2]].sum(axis=1)\n",
    "    dfs.append(df)\n",
    "    configs.append({\n",
    "        \"target\":target,\n",
    "        \"MAXCYCLE\":10,\n",
    "        \"model_type\": model_type,\n",
    "        \"exp\":exp,\n",
    "        \"data_dir\": data[\"data_dir\"]\n",
    "    })\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc113b0-8f50-457a-9550-328e5e4a624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = {\"target_0\": \"5A12_Ang2_fitness_4ZFG\", \"target_1\": \"5A12_VEGF_fitness_4ZFF\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01577806-c3d9-483b-9150-896c8247aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_ddg_dfs={}\n",
    "sampled_seq_dfs = {}\n",
    "flex_ddg_df_alls = {}\n",
    "for target in test_targets.values():\n",
    "    for mode in [\"bias\", \"unbias\"]:\n",
    "        flex_ddg_df = pd.read_csv(f\"flexddgs/{target}/{mode}/outputs-results.csv\")\n",
    "        flex_ddg_df = flex_ddg_df[flex_ddg_df[\"scored_state\"]==\"ddG\"].groupby(\"case_name\")[\"total_score\"].min().sort_index()\n",
    "        flex_ddg_dfs[target+\"_\"+mode]=flex_ddg_df\n",
    "        sampled_seq_dfs[target+\"_\"+mode]=pd.read_csv(f\"flexddgs/{target}/{mode}/sampled_mutations.csv\", index_col=0)\n",
    "\n",
    "test_dfs = {target: pd.read_csv(f\"flexddgs/{target}/bias/sampled_mutations.csv\") for target in test_targets.values()}\n",
    "for target in test_dfs:\n",
    "    test_dfs[target][\"DMS_score\"] = - flex_ddg_dfs[target+\"_bias\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390c8ea-339b-46c8-8623-ae236cee1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_point = [2,2,1]\n",
    "score_cols = [\"flxddg_0_std\", \"flxddg_1_std\", \"ablang2_perplexity_std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ff042-5992-415b-a4f9-96c55558ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pareto import is_pareto_front, nondominated_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724e00f-1303-4501-9b42-c77d1ea29ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "N=40\n",
    "\n",
    "all_df_merges=[]\n",
    "filter_df_merges=[]\n",
    "top_df_merges_0=[]\n",
    "top_df_merges_1=[]\n",
    "non_dominated_df_merges=[]\n",
    "hv_df_merges=[]\n",
    "dual_df_merges=[]\n",
    "sum_df_merges=[]\n",
    "cycle_df_merges=[]\n",
    "for i in range(len(dfs)):\n",
    "    target=configs[i][\"target\"]\n",
    "    exp=configs[i][\"exp\"]\n",
    "    df = dfs[i]\n",
    "    CYCLE=configs[i][\"MAXCYCLE\"]\n",
    "    top_dfs_0 = {cycle+1: df[df[\"cycle\"]<=cycle].sort_values(\"DMS_score_0\", ascending=False).head(N)\n",
    "               for cycle in range(CYCLE)}\n",
    "    top_dfs_1 = {cycle+1: df[df[\"cycle\"]<=cycle].sort_values(\"DMS_score_1\", ascending=False).head(N)\n",
    "               for cycle in range(CYCLE)}\n",
    "    all_dfs = {cycle+1: df[df[\"cycle\"]<=cycle] for cycle in range(CYCLE)}\n",
    "\n",
    "    cycle_dfs = {cycle+1: df[df[\"cycle\"]==cycle] for cycle in range(CYCLE)}\n",
    "    # Sum filtering\n",
    "    sum_dfs = {cycle+1: df[df[\"cycle\"]<=cycle].sort_values(\"sum_score\", ascending=True).head(N)\n",
    "               for cycle in range(CYCLE)}\n",
    "\n",
    "    # Filter filtering\n",
    "    filter_dfs = {}\n",
    "    for cycle in range(CYCLE):\n",
    "        cycle_df = df[df[\"cycle\"]<=cycle].copy()\n",
    "        cycle_df = cycle_df[cycle_df[\"ablang2_perplexity\"]<10]\n",
    "        # cycle_df = cycle_df[cycle_df[\"IP_seq\"]>6]\n",
    "        ranks = nondominated_rank(cycle_df[score_cols[:2]].values)\n",
    "        filter_dfs[cycle+1] = cycle_df.iloc[np.argsort(ranks)][:N]\n",
    "\n",
    "    # Non-dominated filtering\n",
    "    non_dominated_dfs = {}\n",
    "    for cycle in range(CYCLE):\n",
    "        cycle_df = df[df[\"cycle\"]<=cycle]\n",
    "        ranks = nondominated_rank(cycle_df[score_cols[:2]].values)\n",
    "        non_dominated_dfs[cycle+1] = cycle_df.iloc[np.argsort(ranks)][:N]\n",
    "\n",
    "    df_c = df.copy()\n",
    "    for i in range(len(score_cols)):\n",
    "        df_c = df_c[df_c[score_cols[i]] <= ref_point[i]]\n",
    "    hv_dfs = {}\n",
    "    for cycle in range(CYCLE):\n",
    "        cycle_df = df_c[df_c[\"cycle\"]<=cycle]\n",
    "        selected_indices, _ = greedy_hypervolume_subset(cycle_df[score_cols].values, N, ref_point)\n",
    "        hv_dfs[cycle+1] = cycle_df.iloc[selected_indices]\n",
    "\n",
    "    df_c = df.copy()\n",
    "    for i in range(2):\n",
    "        df_c = df_c[df_c[score_cols[i]] <= ref_point[i]]\n",
    "\n",
    "    dual_dfs = {}\n",
    "    for cycle in range(CYCLE):\n",
    "        cycle_df = df_c[df_c[\"cycle\"]<=cycle]\n",
    "        selected_indices, _ = greedy_hypervolume_subset(cycle_df[score_cols[:2]].values, N, ref_point[:2])\n",
    "        dual_dfs[cycle+1] = cycle_df.iloc[selected_indices]\n",
    "\n",
    "    top_df_merge_0 = pd.concat(top_dfs_0)\n",
    "    top_df_merge_1 = pd.concat(top_dfs_1)\n",
    "    all_df_merge = pd.concat(all_dfs)\n",
    "    hv_df_merge = pd.concat(hv_dfs)\n",
    "    dual_df_merge = pd.concat(dual_dfs)\n",
    "    sum_df_merge = pd.concat(sum_dfs)\n",
    "    filter_df_merge = pd.concat(filter_dfs)\n",
    "    non_dominated_df_merge = pd.concat(non_dominated_dfs)\n",
    "    cycle_df_merge = pd.concat(cycle_dfs)\n",
    "\n",
    "    top_df_merge_0.index.names=[\"CYCLE\", \"index\"]\n",
    "    top_df_merge_1.index.names=[\"CYCLE\", \"index\"]\n",
    "    all_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    hv_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    dual_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    sum_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    filter_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    non_dominated_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    cycle_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "\n",
    "    top_df_merge_0 = top_df_merge_0.reset_index()\n",
    "    top_df_merge_1 = top_df_merge_1.reset_index()\n",
    "    all_df_merge = all_df_merge.reset_index()\n",
    "    hv_df_merge = hv_df_merge.reset_index()\n",
    "    dual_df_merge = dual_df_merge.reset_index()\n",
    "    sum_df_merge = sum_df_merge.reset_index()\n",
    "    filter_df_merge = filter_df_merge.reset_index()\n",
    "    non_dominated_df_merge = non_dominated_df_merge.reset_index()\n",
    "    cycle_df_merge = cycle_df_merge.reset_index()\n",
    "\n",
    "    top_df_merges_0.append(top_df_merge_0)\n",
    "    top_df_merges_1.append(top_df_merge_1)\n",
    "    all_df_merges.append(all_df_merge)\n",
    "    hv_df_merges.append(hv_df_merge)\n",
    "    dual_df_merges.append(dual_df_merge)\n",
    "    sum_df_merges.append(sum_df_merge)\n",
    "    filter_df_merges.append(filter_df_merge)\n",
    "    non_dominated_df_merges.append(non_dominated_df_merge)\n",
    "    cycle_df_merges.append(cycle_df_merge)\n",
    "\n",
    "top_df_merge_cat_0 = pd.concat(top_df_merges_0)\n",
    "top_df_merge_cat_1 = pd.concat(top_df_merges_1)\n",
    "all_df_merge_cat = pd.concat(all_df_merges)\n",
    "hv_df_merge_cat = pd.concat(hv_df_merges)\n",
    "dual_df_merge_cat = pd.concat(dual_df_merges)\n",
    "sum_df_merge_cat = pd.concat(sum_df_merges)\n",
    "filter_df_merge_cat = pd.concat(filter_df_merges)\n",
    "non_dominated_df_merge_cat = pd.concat(non_dominated_df_merges)\n",
    "cycle_df_merge_cat = pd.concat(cycle_df_merges)\n",
    "\n",
    "all_divs = []\n",
    "top_divs_0 = []\n",
    "top_divs_1 = []\n",
    "hv_divs = []\n",
    "dual_divs = []\n",
    "sum_divs = []\n",
    "filter_divs = []\n",
    "non_dominated_divs = []\n",
    "cycle_divs = []\n",
    "for i in range(len(dfs)):\n",
    "    alldf = all_df_merges[i]\n",
    "    topdf0 = top_df_merges_0[i]\n",
    "    topdf1 = top_df_merges_1[i]\n",
    "    hvdf = hv_df_merges[i]\n",
    "    dualdf = dual_df_merges[i]\n",
    "    sumdf = sum_df_merges[i]\n",
    "    filterdf = filter_df_merges[i]\n",
    "    non_dominateddf = non_dominated_df_merges[i]\n",
    "    cycledf = cycle_df_merges[i]\n",
    "\n",
    "    conf=configs[i]\n",
    "    input_dir = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], \"9\", \"target_0\", \"train_data\")\n",
    "    emb = np.load(os.path.join(input_dir, \"embedding.npy\"))\n",
    "    # emb = np.load(os.path.join(input_dir, \"embedding_umap5.npy\"))\n",
    "\n",
    "    # Calculate diversity metrics for all sequences\n",
    "    divs = {cycle: 1-calculate_mean_similarity(emb[alldf[(alldf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    mean_muts = pd.Series({cycle: alldf[alldf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    med_muts = pd.Series({cycle: alldf[alldf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    divs = pd.Series(divs)\n",
    "    divs.index.name=\"CYCLE\"\n",
    "    divs.name=\"Diversity\"\n",
    "    divs = divs.reset_index()\n",
    "    divs[\"mean_mutation_num\"] = mean_muts.values\n",
    "    divs[\"median_mutation_num\"] = med_muts.values\n",
    "    divs[\"target\"]=conf[\"target\"]\n",
    "    divs[\"model_type\"]=conf[\"model_type\"]\n",
    "    divs[\"exp\"]=conf[\"exp\"]\n",
    "    all_divs.append(divs)\n",
    "\n",
    "    # Calculate diversity metrics for top sequences\n",
    "    top_div_0 = {cycle: 1-calculate_mean_similarity(emb[topdf0[(topdf0[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    top_mean_muts_0 = pd.Series({cycle: topdf0[topdf0[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    top_med_muts_0 = pd.Series({cycle: topdf0[topdf0[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    top_div_0 = pd.Series(top_div_0)\n",
    "    top_div_0.index.name=\"CYCLE\"\n",
    "    top_div_0.name=\"Diversity\"\n",
    "    top_div_0 = top_div_0.reset_index()\n",
    "    top_div_0[\"mean_mutation_num\"] = top_mean_muts_0.values\n",
    "    top_div_0[\"median_mutation_num\"] = top_med_muts_0.values\n",
    "    top_div_0[\"target\"]=conf[\"target\"]\n",
    "    top_div_0[\"model_type\"]=conf[\"model_type\"]\n",
    "    top_div_0[\"exp\"]=conf[\"exp\"]\n",
    "    top_divs_0.append(top_div_0)\n",
    "\n",
    "    top_div_1 = {cycle: 1-calculate_mean_similarity(emb[topdf1[(topdf1[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    top_mean_muts_1 = pd.Series({cycle: topdf1[topdf1[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    top_med_muts_1 = pd.Series({cycle: topdf1[topdf1[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    top_div_1 = pd.Series(top_div_1)\n",
    "    top_div_1.index.name=\"CYCLE\"\n",
    "    top_div_1.name=\"Diversity\"\n",
    "    top_div_1 = top_div_1.reset_index()\n",
    "    top_div_1[\"mean_mutation_num\"] = top_mean_muts_1.values\n",
    "    top_div_1[\"median_mutation_num\"] = top_med_muts_1.values\n",
    "    top_div_1[\"target\"]=conf[\"target\"]\n",
    "    top_div_1[\"model_type\"]=conf[\"model_type\"]\n",
    "    top_div_1[\"exp\"]=conf[\"exp\"]\n",
    "    top_divs_1.append(top_div_1)\n",
    "\n",
    "    # Calculate diversity metrics for hypervolume sequences\n",
    "    hv_div = {cycle: 1-calculate_mean_similarity(emb[hvdf[(hvdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    hv_mean_muts = pd.Series({cycle: hvdf[hvdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    hv_med_muts = pd.Series({cycle: hvdf[hvdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    hv_div = pd.Series(hv_div)\n",
    "    hv_div.index.name=\"CYCLE\"\n",
    "    hv_div.name=\"Diversity\"\n",
    "    hv_div = hv_div.reset_index()\n",
    "    hv_div[\"mean_mutation_num\"] = hv_mean_muts.values\n",
    "    hv_div[\"median_mutation_num\"] = hv_med_muts.values\n",
    "    hv_div[\"target\"]=conf[\"target\"]\n",
    "    hv_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    hv_div[\"exp\"]=conf[\"exp\"]\n",
    "    hv_divs.append(hv_div)\n",
    "\n",
    "    # Calculate diversity metrics for dual sequences\n",
    "    dual_div = {cycle: 1-calculate_mean_similarity(emb[dualdf[(dualdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    dual_mean_muts = pd.Series({cycle: dualdf[dualdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    dual_med_muts = pd.Series({cycle: dualdf[dualdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    dual_div = pd.Series(dual_div)\n",
    "    dual_div.index.name=\"CYCLE\"\n",
    "    dual_div.name=\"Diversity\"\n",
    "    dual_div = dual_div.reset_index()\n",
    "    dual_div[\"mean_mutation_num\"] = dual_mean_muts.values\n",
    "    dual_div[\"median_mutation_num\"] = dual_med_muts.values\n",
    "    dual_div[\"target\"]=conf[\"target\"]\n",
    "    dual_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    dual_div[\"exp\"]=conf[\"exp\"]\n",
    "    dual_divs.append(dual_div)\n",
    "\n",
    "    # Calculate diversity metrics for sum sequences\n",
    "    sum_div = {cycle: 1-calculate_mean_similarity(emb[sumdf[(sumdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    sum_mean_muts = pd.Series({cycle: sumdf[sumdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    sum_med_muts = pd.Series({cycle: sumdf[sumdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    sum_div = pd.Series(sum_div)\n",
    "    sum_div.index.name=\"CYCLE\"\n",
    "    sum_div.name=\"Diversity\"\n",
    "    sum_div = sum_div.reset_index()\n",
    "    sum_div[\"mean_mutation_num\"] = sum_mean_muts.values\n",
    "    sum_div[\"median_mutation_num\"] = sum_med_muts.values\n",
    "    sum_div[\"target\"]=conf[\"target\"]\n",
    "    sum_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    sum_div[\"exp\"]=conf[\"exp\"]\n",
    "    sum_divs.append(sum_div)\n",
    "\n",
    "    # Calculate diversity metrics for filtered sequences\n",
    "    filter_div = {cycle: 1-calculate_mean_similarity(emb[filterdf[(filterdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    filter_mean_muts = pd.Series({cycle: filterdf[filterdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    filter_med_muts = pd.Series({cycle: filterdf[filterdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    filter_div = pd.Series(filter_div)\n",
    "    filter_div.index.name=\"CYCLE\"\n",
    "    filter_div.name=\"Diversity\"\n",
    "    filter_div = filter_div.reset_index()\n",
    "    filter_div[\"mean_mutation_num\"] = filter_mean_muts.values\n",
    "    filter_div[\"median_mutation_num\"] = filter_med_muts.values\n",
    "    filter_div[\"target\"]=conf[\"target\"]\n",
    "    filter_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    filter_div[\"exp\"]=conf[\"exp\"]\n",
    "    filter_divs.append(filter_div)\n",
    "\n",
    "    # Calculate diversity metrics for non-dominated sequences\n",
    "    non_dominated_div = {cycle: 1-calculate_mean_similarity(emb[non_dominateddf[(non_dominateddf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    non_dominated_mean_muts = pd.Series({cycle: non_dominateddf[non_dominateddf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    non_dominated_med_muts = pd.Series({cycle: non_dominateddf[non_dominateddf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    non_dominated_div = pd.Series(non_dominated_div)\n",
    "    non_dominated_div.index.name=\"CYCLE\"\n",
    "    non_dominated_div.name=\"Diversity\"\n",
    "    non_dominated_div = non_dominated_div.reset_index()\n",
    "    non_dominated_div[\"mean_mutation_num\"] = non_dominated_mean_muts.values\n",
    "    non_dominated_div[\"median_mutation_num\"] = non_dominated_med_muts.values\n",
    "    non_dominated_div[\"target\"]=conf[\"target\"]\n",
    "    non_dominated_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    non_dominated_div[\"exp\"]=conf[\"exp\"]\n",
    "    non_dominated_divs.append(non_dominated_div)\n",
    "\n",
    "    # Calculate diversity metrics for cycle sequences\n",
    "    cycle_div = {cycle: 1-calculate_mean_similarity(emb[cycledf[(cycledf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    cycle_mean_muts = pd.Series({cycle: cycledf[cycledf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    cycle_med_muts = pd.Series({cycle: cycledf[cycledf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    cycle_div = pd.Series(cycle_div)\n",
    "    cycle_div.index.name=\"CYCLE\"\n",
    "    cycle_div.name=\"Diversity\"\n",
    "    cycle_div = cycle_div.reset_index()\n",
    "    cycle_div[\"mean_mutation_num\"] = cycle_mean_muts.values\n",
    "    cycle_div[\"median_mutation_num\"] = cycle_med_muts.values\n",
    "    cycle_div[\"target\"]=conf[\"target\"]\n",
    "    cycle_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    cycle_div[\"exp\"]=conf[\"exp\"]\n",
    "    cycle_divs.append(cycle_div)\n",
    "\n",
    "all_divs_cat = pd.concat(all_divs, ignore_index=True)\n",
    "top_divs_cat_0 = pd.concat(top_divs_0, ignore_index=True)\n",
    "top_divs_cat_1 = pd.concat(top_divs_1, ignore_index=True)\n",
    "hv_divs_cat = pd.concat(hv_divs, ignore_index=True)\n",
    "dual_divs_cat = pd.concat(dual_divs, ignore_index=True)\n",
    "sum_divs_cat = pd.concat(sum_divs, ignore_index=True)\n",
    "filter_divs_cat = pd.concat(filter_divs, ignore_index=True)\n",
    "non_dominated_divs_cat = pd.concat(non_dominated_divs, ignore_index=True)\n",
    "cycle_divs_cat = pd.concat(cycle_divs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81133b6a-4927-4462-80cd-c8a12a2f0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"5A12_dual\", \"5A12_dual_weak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc8a67-e533-43ac-92d8-9c0ce07799cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_scores_0=[]\n",
    "all_test_scores_1=[]\n",
    "for conf in configs:\n",
    "    target = conf[\"target\"]\n",
    "    if target not in targets:\n",
    "        print(target)\n",
    "        continue\n",
    "    for cycle in range(10):\n",
    "        input_dir_0 = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], str(cycle), \"target_0\", \"train_data\")\n",
    "        input_dir_1 = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], str(cycle), \"target_1\", \"train_data\")\n",
    "        test_pred_0 = np.load(os.path.join(input_dir_0, \"test_inference_bias.npy\"))\n",
    "        test_pred_1 = np.load(os.path.join(input_dir_1, \"test_inference_bias.npy\"))\n",
    "        test_df_0 = test_dfs[test_targets[\"target_0\"]].copy()\n",
    "        test_df_0[\"Pred\"] = test_pred_0\n",
    "        test_df_1 = test_dfs[test_targets[\"target_1\"]].copy()\n",
    "        test_df_1[\"Pred\"] = test_pred_1\n",
    "        all_test_scores_0.append({\n",
    "            **calc_test(test_df_0[\"DMS_score\"], test_df_0[\"Pred\"]),\n",
    "            \"CYCLE\": cycle+1,\n",
    "            \"target\": conf[\"target\"],\n",
    "            \"model_type\": conf[\"model_type\"],\n",
    "            \"exp\": conf[\"exp\"],\n",
    "        })\n",
    "        all_test_scores_1.append({\n",
    "            **calc_test(test_df_1[\"DMS_score\"], test_df_1[\"Pred\"]),\n",
    "            \"CYCLE\": cycle+1,\n",
    "            \"target\": conf[\"target\"],\n",
    "            \"model_type\": conf[\"model_type\"],\n",
    "            \"exp\": conf[\"exp\"],\n",
    "        })\n",
    "all_test_scores_cat_0 = pd.DataFrame(all_test_scores_0)\n",
    "all_test_scores_cat_1 = pd.DataFrame(all_test_scores_1)\n",
    "all_test_scores_cat_0[\"spearman\"] = all_test_scores_cat_0[\"spearman\"].fillna(0)\n",
    "all_test_scores_cat_1[\"spearman\"] = all_test_scores_cat_1[\"spearman\"].fillna(0)\n",
    "\n",
    "all_test_scores_cat_0[\"spearman_0\"] = all_test_scores_cat_0[\"spearman\"]\n",
    "all_test_scores_cat_1[\"spearman_1\"] = all_test_scores_cat_1[\"spearman\"]\n",
    "\n",
    "all_test_scores_cat_0[\"ndcg_0\"] = all_test_scores_cat_0[\"ndcg\"]\n",
    "all_test_scores_cat_1[\"ndcg_1\"] = all_test_scores_cat_1[\"ndcg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1ce11-92fe-4900-8e50-c581c6706cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_df_merge_cat.to_csv(\"cycle_df_merge_cat_dual.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664124f7-d999-4821-8250-a02fc09db84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df_merge_cat.to_csv(\"../results/flexddg_online/dual/sum_results.csv\",index=False)\n",
    "all_test_scores_cat_0.to_csv(\"../results/flexddg_online/dual/all_results_test_Ang2.csv\",index=False)\n",
    "all_test_scores_cat_1.to_csv(\"../flexddg_online/dual/all_results_test_VEGF.csv\",index=False)\n",
    "all_df_merge_cat.to_csv(\"../results/flexddg_online/dual/all_results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg39",
   "language": "python",
   "name": "pyg39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
