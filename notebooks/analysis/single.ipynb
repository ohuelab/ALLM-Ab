{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc103e5-fdeb-4f7d-aad3-f63cea837b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046d16b-fdf8-4457-9615-e7073e00a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f5e8f-0e1c-4736-a52e-52c102fb408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, minmax_scale\n",
    "from sklearn.metrics import root_mean_squared_error, ndcg_score\n",
    "def calc_test(true_scores, pred_scores, k=10):\n",
    "    rho, _ = stats.spearmanr(true_scores, pred_scores)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = root_mean_squared_error(true_scores, pred_scores)\n",
    "\n",
    "    # NDCG@k\n",
    "    std_tgts = minmax_scale([true_scores], (0, 5), axis=1)\n",
    "    ndcg_val = ndcg_score(std_tgts,[pred_scores], k=k)\n",
    "\n",
    "    result ={\n",
    "        'spearman': rho,\n",
    "        # 'rmse': rmse,\n",
    "        'ndcg': ndcg_val\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a9599-5428-4df5-bdd0-5acd9ab53591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pygmo import hypervolume\n",
    "\n",
    "def greedy_hypervolume_subset(points, n, ref_point):\n",
    "    selected = []\n",
    "    remaining = list(range(len(points)))\n",
    "\n",
    "    for _ in tqdm(range(n)):\n",
    "        max_hv = -float('inf')\n",
    "        best_idx = None\n",
    "\n",
    "        for idx in remaining:\n",
    "            # 現在の選択 + 候補点のHypervolume計算\n",
    "            current_points = points[selected + [idx]]\n",
    "            hv = hypervolume(current_points)\n",
    "            current_hv = hv.compute(ref_point)\n",
    "\n",
    "            if current_hv > max_hv:\n",
    "                max_hv = current_hv\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_idx is not None:\n",
    "            selected.append(best_idx)\n",
    "            remaining.remove(best_idx)\n",
    "\n",
    "    return selected, max_hv\n",
    "\n",
    "def normalize_score(score):\n",
    "    return (score-score.quantile(0.05))/(score.quantile(0.95)-score.quantile(0.05)+1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83763ecf-d1f0-4aae-986a-553864748f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "def calculate_mean_similarity(latent_matrix: np.ndarray):\n",
    "    \n",
    "    # 入力チェック\n",
    "    if not isinstance(latent_matrix, np.ndarray):\n",
    "        raise TypeError(\"latent_matrix must be numpy.ndarray\")\n",
    "    \n",
    "    if len(latent_matrix.shape) != 2:\n",
    "        raise ValueError(\"latent_matrix must be 2-dimensional\")\n",
    "        \n",
    "    N, H = latent_matrix.shape\n",
    "    \n",
    "    if N < 2:\n",
    "        raise ValueError(\"Number of samples must be greater than 1\")\n",
    "    \n",
    "    # 各ベクトルのノルムを計算\n",
    "    norms = np.linalg.norm(latent_matrix, axis=1, keepdims=True)\n",
    "    # ゼロ除算を防ぐ\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    \n",
    "    # 正規化された行列を計算\n",
    "    normalized_matrix = latent_matrix / norms\n",
    "    \n",
    "    # コサイン類似度行列を計算\n",
    "    similarity_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "    # 対角要素を0にする（自己との類似度は除外）\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    \n",
    "    # 平均コサイン類似度を計算\n",
    "    mean_similarity = similarity_matrix.sum() / (N * (N-1))\n",
    "        \n",
    "    return mean_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d366b20-d0a9-4c06-89dc-e9f6b882894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logomaker\n",
    "def draw_logo(seqs, ax=None):\n",
    "    if isinstance(seqs, str):\n",
    "        seqs = [seqs]\n",
    "    counts_matrix = logomaker.alignment_to_matrix(seqs)\n",
    "    \n",
    "    logo = logomaker.Logo(counts_matrix,\n",
    "            shade_below=.5,\n",
    "            fade_below=.5,\n",
    "            color_scheme='NajafabadiEtAl2017',\n",
    "            ax=ax\n",
    "        )\n",
    "    logo.ax.spines['right'].set_visible(False)\n",
    "    logo.ax.spines['top'].set_visible(False)\n",
    "    logo.ax.spines['bottom'].set_visible(False)\n",
    "    logo.ax.spines['left'].set_visible(False)\n",
    "    # logo.ax.set_xticks(np.arange(length))\n",
    "    logo.ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b993e-8bfb-498a-b6b0-01c645ae229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_dict = {\n",
    "    \"4D5_HER2_fitness_1N8Z\":   \"SRWGGDGFYAMDY\",\n",
    "    \"5A12_Ang2_fitness_4ZFG\":  \"ARFVFFLPYAMDY\",\n",
    "    \"5A12_VEGF_fitness_4ZFF\":  \"ARFVFFLPYAMDY\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c20fc-0e39-4431-8267-8a1fc794004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ddg(path):\n",
    "    results_df = pd.read_csv(path)\n",
    "    \n",
    "    ddg_scores = (results_df[results_df[\"scored_state\"]==\"ddG\"]\n",
    "                     .groupby(\"case_name\")[\"total_score\"]\n",
    "                     .min()\n",
    "                     .sort_index())\n",
    "    return ddg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b5524-c23f-4b2c-a0ed-19d2d9044362",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_ddg_dfs={}\n",
    "sampled_seq_dfs = {}\n",
    "flex_ddg_df_alls = {}\n",
    "for target in targets:\n",
    "    for mode in [\"bias\", \"unbias\"]:\n",
    "        flex_ddg_df = pd.read_csv(f\"flexddgs/{target}/{mode}/outputs-results.csv\")\n",
    "        flex_ddg_df = flex_ddg_df[flex_ddg_df[\"scored_state\"]==\"ddG\"].groupby(\"case_name\")[\"total_score\"].min().sort_index()\n",
    "        flex_ddg_dfs[target+\"_\"+mode]=flex_ddg_df\n",
    "        sampled_seq_dfs[target+\"_\"+mode]=pd.read_csv(f\"flexddgs/{target}/{mode}/sampled_mutations.csv\", index_col=0)\n",
    "        \n",
    "test_dfs = {target: pd.read_csv(f\"flexddgs/{target}/bias/sampled_mutations.csv\") for target in targets}\n",
    "for target in test_dfs:\n",
    "    test_dfs[target][\"DMS_score\"] = - flex_ddg_dfs[target+\"_bias\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc6430-3dbc-4273-8c5d-1a585d8f8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1846a-bf6f-415d-91f1-9d84d0e0e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobdf = pd.read_csv(\"jobs/job.csv\")\n",
    "import os\n",
    "cycles = {}\n",
    "dfs = []\n",
    "configs = []\n",
    "ref_points = []\n",
    "for confpath in jobdf[\"CONFIG\"]:\n",
    "    with open(confpath) as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    target=data[\"data_dir\"].split(\"/\")[1]\n",
    "    model_type = data[\"data_dir\"].split(\"/\")[2]\n",
    "    exp=data[\"data_dir\"].split(\"/\")[3]\n",
    "    df = pd.read_csv(os.path.join(data_dir, \"..\", data[\"data_dir\"], \"9\", \"train_data\", \"training_data.csv\"))\n",
    "    df[\"target\"]=target\n",
    "    df[\"model_type\"]=model_type\n",
    "    df[\"mutations\"] = df[\"mutations\"].fillna(\"\")\n",
    "    # df[\"mutations_wt\"] = df[\"mutseq\"].apply(lambda x: mutseq_to_mutstr(x, h3_dict[\"4D5_HER2_fitness_1N8Z\"], \"B\", offset=0))\n",
    "    df[\"exp\"]=exp\n",
    "    df[\"flxddg\"] = -df[\"DMS_score\"]\n",
    "    score_cols = [\"flxddg_std\", \"ablang2_perplexity_std\", \"IP_seq_std\"]\n",
    "\n",
    "    # hv\n",
    "    df[\"flxddg_std\"] = normalize_score(df[\"flxddg\"])\n",
    "    df[\"ablang2_perplexity_std\"] = normalize_score(df[\"ablang2_perplexity\"])\n",
    "    df[\"IP_seq_std\"] = normalize_score(-df[\"IP_seq\"])\n",
    "    \n",
    "    df[\"flxddg_std\"]*=2\n",
    "    ref_points\n",
    "    ref_point = []\n",
    "    for col in score_cols:\n",
    "        ref_point.append(df[col].quantile(0.95))\n",
    "    ref_points.append(ref_point)\n",
    "    # df[\"hv_score\"] = df[score_cols].sum(axis=1)\n",
    "    \n",
    "    df[\"#Mutation\"]=df[\"mutations\"].apply(lambda x: len(x.split(\",\")) if x !=\"\" else 0)\n",
    "    dfs.append(df)\n",
    "    configs.append({\n",
    "        \"target\":target,\n",
    "        \"MAXCYCLE\":10,\n",
    "        \"model_type\": model_type,\n",
    "        \"exp\":exp,\n",
    "        \"data_dir\": data[\"data_dir\"]\n",
    "        \n",
    "    })\n",
    "    ddgs_list = []\n",
    "    for cycle in range(10):\n",
    "        ddgs = get_ddg(os.path.join(data_dir, \"..\", data[\"data_dir\"], str(cycle), \"flex_ddG\", \"outputs-results.csv\"))\n",
    "        ddgs = ddgs.reset_index()\n",
    "        ddgs[\"cycle\"]=cycle\n",
    "        ddgs_list.append(ddgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e8e12-8483-45de-a40a-7f62e4dfb08b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "N=40\n",
    "\n",
    "all_df_merges=[]\n",
    "top_df_merges=[]\n",
    "hv_df_merges=[]\n",
    "for i in range(len(dfs)):\n",
    "    target=configs[i][\"target\"]\n",
    "    exp=configs[i][\"exp\"]\n",
    "    df = dfs[i]\n",
    "    CYCLE=configs[i][\"MAXCYCLE\"]\n",
    "    top_dfs = {cycle+1: df[df[\"cycle\"]<=cycle].sort_values(\"DMS_score\", ascending=False).head(N)\n",
    "               for cycle in range(CYCLE)}\n",
    "    all_dfs = {cycle+1: df[df[\"cycle\"]<=cycle] for cycle in range(CYCLE)}\n",
    "    df_c = df.copy()\n",
    "    for i in range(3):\n",
    "        df_c = df_c[df_c[score_cols[i]] <= ref_point[i]]\n",
    "\n",
    "\n",
    "    hv_dfs = {}\n",
    "    for cycle in range(CYCLE):\n",
    "        cycle_df = df_c[df_c[\"cycle\"]<=cycle]\n",
    "        selected_indices, _ = greedy_hypervolume_subset(cycle_df[score_cols].values, N, ref_point)\n",
    "        hv_dfs[cycle+1] = cycle_df.iloc[selected_indices]\n",
    "\n",
    "    top_df_merge = pd.concat(top_dfs)\n",
    "    all_df_merge = pd.concat(all_dfs)\n",
    "    hv_df_merge = pd.concat(hv_dfs)\n",
    "    top_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    all_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    hv_df_merge.index.names=[\"CYCLE\", \"index\"]\n",
    "    top_df_merge = top_df_merge.reset_index()\n",
    "    all_df_merge = all_df_merge.reset_index()\n",
    "    hv_df_merge = hv_df_merge.reset_index()\n",
    "    top_df_merges.append(top_df_merge)\n",
    "    all_df_merges.append(all_df_merge)\n",
    "    hv_df_merges.append(hv_df_merge)\n",
    "top_df_merge_cat = pd.concat(top_df_merges)\n",
    "all_df_merge_cat = pd.concat(all_df_merges)\n",
    "hv_df_merge_cat = pd.concat(hv_df_merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9896c4-7390-4a91-9dfe-888b02fa0f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_divs = []\n",
    "top_divs = []\n",
    "hv_divs = []\n",
    "for i in range(len(dfs)):\n",
    "    alldf = all_df_merges[i]\n",
    "    topdf = top_df_merges[i]\n",
    "    hvdf = hv_df_merges[i]\n",
    "\n",
    "    conf=configs[i]\n",
    "    input_dir = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], \"9\", \"train_data\")\n",
    "    emb = np.load(os.path.join(input_dir, \"embedding.npy\"))\n",
    "    # emb = np.load(os.path.join(input_dir, \"embedding_umap5.npy\"))\n",
    "    \n",
    "    # Calculate diversity metrics for all sequences\n",
    "    divs = {cycle: 1-calculate_mean_similarity(emb[alldf[(alldf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    mean_muts = pd.Series({cycle: alldf[alldf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    med_muts = pd.Series({cycle: alldf[alldf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    divs = pd.Series(divs)\n",
    "    divs.index.name=\"CYCLE\"\n",
    "    divs.name=\"Diversity\"\n",
    "    divs = divs.reset_index()\n",
    "    divs[\"mean_mutation_num\"] = mean_muts.values\n",
    "    divs[\"median_mutation_num\"] = med_muts.values\n",
    "    divs[\"target\"]=conf[\"target\"]\n",
    "    divs[\"model_type\"]=conf[\"model_type\"]\n",
    "    divs[\"exp\"]=conf[\"exp\"]\n",
    "    all_divs.append(divs)\n",
    "\n",
    "    # Calculate diversity metrics for top sequences\n",
    "    top_div = {cycle: 1-calculate_mean_similarity(emb[topdf[(topdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    top_mean_muts = pd.Series({cycle: topdf[topdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    top_med_muts = pd.Series({cycle: topdf[topdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    top_div = pd.Series(top_div)\n",
    "    top_div.index.name=\"CYCLE\"\n",
    "    top_div.name=\"Diversity\"\n",
    "    top_div = top_div.reset_index()\n",
    "    top_div[\"mean_mutation_num\"] = top_mean_muts.values\n",
    "    top_div[\"median_mutation_num\"] = top_med_muts.values\n",
    "    top_div[\"target\"]=conf[\"target\"]\n",
    "    top_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    top_div[\"exp\"]=conf[\"exp\"]\n",
    "    top_divs.append(top_div)\n",
    "\n",
    "    # Calculate diversity metrics for hypervolume sequences\n",
    "    hv_div = {cycle: 1-calculate_mean_similarity(emb[hvdf[(hvdf[\"CYCLE\"]==cycle)][\"index\"].values]) for cycle in range(1,11)}\n",
    "    hv_mean_muts = pd.Series({cycle: hvdf[hvdf[\"CYCLE\"]==cycle][\"#Mutation\"].mean() for cycle in range(1,11)})\n",
    "    hv_med_muts = pd.Series({cycle: hvdf[hvdf[\"CYCLE\"]==cycle][\"#Mutation\"].median() for cycle in range(1,11)})\n",
    "    hv_div = pd.Series(hv_div)\n",
    "    hv_div.index.name=\"CYCLE\"\n",
    "    hv_div.name=\"Diversity\"\n",
    "    hv_div = hv_div.reset_index()\n",
    "    hv_div[\"mean_mutation_num\"] = hv_mean_muts.values\n",
    "    hv_div[\"median_mutation_num\"] = hv_med_muts.values\n",
    "    hv_div[\"target\"]=conf[\"target\"]\n",
    "    hv_div[\"model_type\"]=conf[\"model_type\"]\n",
    "    hv_div[\"exp\"]=conf[\"exp\"]\n",
    "    hv_divs.append(hv_div)\n",
    "\n",
    "all_divs_cat = pd.concat(all_divs, ignore_index=True)\n",
    "top_divs_cat = pd.concat(top_divs, ignore_index=True)\n",
    "hv_divs_cat = pd.concat(hv_divs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534e635-37f4-4a7b-abf9-803eead4d5bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_test_scores=[]\n",
    "for conf in configs:\n",
    "    target = conf[\"target\"]\n",
    "    if target not in targets:\n",
    "        continue\n",
    "    for cycle in range(10):\n",
    "        input_dir = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], str(cycle), \"train_data\")\n",
    "        test_pred = np.load(os.path.join(input_dir, \"test_inference_bias.npy\"))\n",
    "        test_df = test_dfs[target]\n",
    "        test_df_ = test_df.copy()\n",
    "        test_df_[\"Pred\"] = test_pred\n",
    "        all_test_scores.append({\n",
    "            **calc_test(test_df_[\"DMS_score\"], test_df_[\"Pred\"]),\n",
    "            \"CYCLE\": cycle+1,\n",
    "            \"target\": conf[\"target\"],\n",
    "            \"model_type\": conf[\"model_type\"],\n",
    "            \"exp\": conf[\"exp\"],\n",
    "        })\n",
    "all_test_scores_cat = pd.DataFrame(all_test_scores)\n",
    "all_test_scores_cat[\"spearman\"] = all_test_scores_cat[\"spearman\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d67944-16bf-4286-9891-0ee521fa2237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_df_merge_cat_sel.to_csv(\"results/flexddg_offline/top_results.csv\",index=False)\n",
    "all_test_scores_cat_sel.to_csv(\"results/flexddg_offline/all_results_test.csv\",index=False)\n",
    "all_df_merge_cat_sel.to_csv(\"results/flexddg_offline/all_results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg39",
   "language": "python",
   "name": "pyg39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
