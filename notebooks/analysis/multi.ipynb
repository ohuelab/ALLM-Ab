{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854ee9c-e83e-4359-a769-42e1a73e4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ddb17-d485-4995-80c8-847bfe2c7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9629e41-6ddd-4fe0-83dd-1ec34b4aa71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, minmax_scale\n",
    "from sklearn.metrics import root_mean_squared_error, ndcg_score\n",
    "def calc_test(true_scores, pred_scores, k=10):\n",
    "    rho, _ = stats.spearmanr(true_scores, pred_scores)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = root_mean_squared_error(true_scores, pred_scores)\n",
    "\n",
    "    # NDCG@k\n",
    "    std_tgts = minmax_scale([true_scores], (0, 5), axis=1)\n",
    "    ndcg_val = ndcg_score(std_tgts,[pred_scores], k=k)\n",
    "\n",
    "    result ={\n",
    "        'spearman': rho,\n",
    "        # 'rmse': rmse,\n",
    "        'ndcg': ndcg_val\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bac77-bf37-4eb7-8498-e1f33509b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pygmo import hypervolume\n",
    "\n",
    "def greedy_hypervolume_subset(points, n, ref_point):\n",
    "    selected = []\n",
    "    remaining = list(range(len(points)))\n",
    "\n",
    "    for _ in range(n):\n",
    "        max_hv = -float('inf')\n",
    "        best_idx = None\n",
    "\n",
    "        for idx in remaining:\n",
    "            # 現在の選択 + 候補点のHypervolume計算\n",
    "            current_points = points[selected + [idx]]\n",
    "            hv = hypervolume(current_points)\n",
    "            current_hv = hv.compute(ref_point)\n",
    "\n",
    "            if current_hv > max_hv:\n",
    "                max_hv = current_hv\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_idx is not None:\n",
    "            selected.append(best_idx)\n",
    "            remaining.remove(best_idx)\n",
    "\n",
    "    return selected, max_hv\n",
    "\n",
    "def normalize_score(score):\n",
    "    return (score-score.quantile(0.05))/(score.quantile(0.95)-score.quantile(0.05)+1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dc216-5705-47d7-9289-bb3619fdd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "def calculate_mean_similarity(latent_matrix: np.ndarray):\n",
    "    \n",
    "    # 入力チェック\n",
    "    if not isinstance(latent_matrix, np.ndarray):\n",
    "        raise TypeError(\"latent_matrix must be numpy.ndarray\")\n",
    "    \n",
    "    if len(latent_matrix.shape) != 2:\n",
    "        raise ValueError(\"latent_matrix must be 2-dimensional\")\n",
    "        \n",
    "    N, H = latent_matrix.shape\n",
    "    \n",
    "    if N < 2:\n",
    "        raise ValueError(\"Number of samples must be greater than 1\")\n",
    "    \n",
    "    # 各ベクトルのノルムを計算\n",
    "    norms = np.linalg.norm(latent_matrix, axis=1, keepdims=True)\n",
    "    # ゼロ除算を防ぐ\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    \n",
    "    # 正規化された行列を計算\n",
    "    normalized_matrix = latent_matrix / norms\n",
    "    \n",
    "    # コサイン類似度行列を計算\n",
    "    similarity_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "    # 対角要素を0にする（自己との類似度は除外）\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    \n",
    "    # 平均コサイン類似度を計算\n",
    "    mean_similarity = similarity_matrix.sum() / (N * (N-1))\n",
    "        \n",
    "    return mean_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834ff6a-eea1-4209-bc3c-dc329373c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logomaker\n",
    "def draw_logo(seqs, ax=None):\n",
    "    if isinstance(seqs, str):\n",
    "        seqs = [seqs]\n",
    "    counts_matrix = logomaker.alignment_to_matrix(seqs)\n",
    "    \n",
    "    logo = logomaker.Logo(counts_matrix,\n",
    "            shade_below=.5,\n",
    "            fade_below=.5,\n",
    "            color_scheme='NajafabadiEtAl2017',\n",
    "            ax=ax\n",
    "        )\n",
    "    logo.ax.spines['right'].set_visible(False)\n",
    "    logo.ax.spines['top'].set_visible(False)\n",
    "    logo.ax.spines['bottom'].set_visible(False)\n",
    "    logo.ax.spines['left'].set_visible(False)\n",
    "    # logo.ax.set_xticks(np.arange(length))\n",
    "    logo.ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0ed1f-4c67-4ac6-853d-d1f1ed3a3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_dict = {\n",
    "    \"4D5_HER2_fitness_1N8Z\":   \"SRWGGDGFYAMDY\",\n",
    "    \"5A12_Ang2_fitness_4ZFG\":  \"ARFVFFLPYAMDY\",\n",
    "    \"5A12_VEGF_fitness_4ZFF\":  \"ARFVFFLPYAMDY\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c5fba-6726-4d4a-b9af-de97df438d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ddg(path):\n",
    "    results_df = pd.read_csv(path)\n",
    "    \n",
    "    ddg_scores = (results_df[results_df[\"scored_state\"]==\"ddG\"]\n",
    "                     .groupby(\"case_name\")[\"total_score\"]\n",
    "                     .min()\n",
    "                     .sort_index())\n",
    "    return ddg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65125db2-4d7a-471b-9dfd-3d4741ccea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\"greedy\", \"greedy_multi\", \"greedy_0.5\", \"greedy_multi_A\", \"greedy_multi_B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951cbed-b7d6-4344-a812-f65c01beb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size=15\n",
    "targets = [\"4D5_HER2_fitness_1N8Z\",\"5A12_Ang2_fitness_4ZFG\", \"5A12_VEGF_fitness_4ZFF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eed00-5d4d-4a90-b09a-a42bff5ea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_ddg_dfs={}\n",
    "sampled_seq_dfs = {}\n",
    "flex_ddg_df_alls = {}\n",
    "for target in targets:\n",
    "    for mode in [\"bias\", \"unbias\"]:\n",
    "        flex_ddg_df = pd.read_csv(f\"flexddgs/{target}/{mode}/outputs-results.csv\")\n",
    "        flex_ddg_df = flex_ddg_df[flex_ddg_df[\"scored_state\"]==\"ddG\"].groupby(\"case_name\")[\"total_score\"].min().sort_index()\n",
    "        flex_ddg_dfs[target+\"_\"+mode]=flex_ddg_df\n",
    "        sampled_seq_dfs[target+\"_\"+mode]=pd.read_csv(f\"flexddgs/{target}/{mode}/sampled_mutations.csv\", index_col=0)\n",
    "        \n",
    "test_dfs = {target: pd.read_csv(f\"flexddgs/{target}/bias/sampled_mutations.csv\") for target in targets}\n",
    "for target in test_dfs:\n",
    "    test_dfs[target][\"DMS_score\"] = - flex_ddg_dfs[target+\"_bias\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edb70e-43a7-474c-8c3a-0f8d6a43b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6d604-d6e8-4d71-b219-d007ce80f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = ['flxddg_std', 'ablang2_perplexity_std']\n",
    "ref_points = [2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e933b2-7209-49b3-9ae7-6e6a623813f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobdf = pd.read_csv(\"jobs/job_multi.csv\")\n",
    "import os\n",
    "cycles = {}\n",
    "dfs = []\n",
    "# dfs2 = []\n",
    "configs = []\n",
    "for confpath in jobdf[\"CONFIG\"]:\n",
    "    with open(confpath) as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    target=data[\"data_dir\"].split(\"/\")[1]\n",
    "    model_type = data[\"data_dir\"].split(\"/\")[2]\n",
    "    exp=data[\"data_dir\"].split(\"/\")[3]\n",
    "    df = pd.read_csv(os.path.join(data_dir, \"..\", data[\"data_dir\"], \"9\", \"train_data\", \"training_data.csv\"))\n",
    "    df[\"target\"]=target\n",
    "    df[\"model_type\"]=model_type\n",
    "    df[\"mutations\"] = df[\"mutations\"].fillna(\"\")\n",
    "    df[\"exp\"]=exp\n",
    "    df[\"flxddg\"] = -df[\"DMS_score\"]\n",
    "\n",
    "    # hv\n",
    "    df[\"flxddg_std\"] = normalize_score(df[\"flxddg\"])\n",
    "    df[\"ablang2_perplexity_std\"] = normalize_score(df[\"ablang2_perplexity\"])\n",
    "    df[\"IP_seq_std\"] = normalize_score(-df[\"IP_seq\"])\n",
    "    for score_col, ref_point in zip(score_cols, ref_points):\n",
    "        df[score_col]*=ref_point\n",
    "    df[\"sum_score\"] =df[score_cols].sum(axis=1)\n",
    "    \n",
    "    df[\"#Mutation\"]=df[\"mutations\"].apply(lambda x: len(x.split(\",\")) if x !=\"\" else 0)\n",
    "    dfs.append(df)\n",
    "    configs.append({\n",
    "        \"target\":target,\n",
    "        \"MAXCYCLE\":10,\n",
    "        \"model_type\": model_type,\n",
    "        \"exp\":exp,\n",
    "        \"data_dir\": data[\"data_dir\"]\n",
    "        \n",
    "    })\n",
    "    ddgs_list = []\n",
    "    for cycle in range(10):\n",
    "        ddgs = get_ddg(os.path.join(data_dir, \"..\", data[\"data_dir\"], str(cycle), \"flex_ddG\", \"outputs-results.csv\"))\n",
    "        ddgs = ddgs.reset_index()\n",
    "        ddgs[\"cycle\"]=cycle\n",
    "        ddgs_list.append(ddgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5e277-5d7a-47c0-a4da-36fd9ad065e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pareto import is_pareto_front, nondominated_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028d268-2a98-49e3-b2dd-fa520fc81d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "N=40\n",
    "\n",
    "def get_filtered_df(df, cycle, filter_type, N=40, score_cols=None, ref_points=None):\n",
    "    cycle_df = df[df[\"cycle\"]<=cycle]\n",
    "    \n",
    "    if filter_type == \"all\":\n",
    "        return cycle_df\n",
    "    elif filter_type == \"cycle\":\n",
    "        return cycle_df[cycle_df[\"cycle\"]==cycle]\n",
    "    elif filter_type == \"top\":\n",
    "        return cycle_df.sort_values(\"DMS_score\", ascending=False).head(N)\n",
    "    elif filter_type == \"filter\":\n",
    "        cycle_df = cycle_df[cycle_df[\"ablang2_perplexity\"]<10].copy()\n",
    "        # cycle_df = cycle_df[cycle_df[\"IP_seq\"]>6]\n",
    "        return cycle_df.sort_values(\"DMS_score\", ascending=False).head(N)\n",
    "    elif filter_type == \"hv\":\n",
    "        df_c = df.copy()\n",
    "        for i in range(len(score_cols)):\n",
    "            df_c = df_c[df_c[score_cols[i]] <= ref_points[i]]\n",
    "        cycle_df = df_c[df_c[\"cycle\"]<=cycle]\n",
    "        selected_indices, _ = greedy_hypervolume_subset(cycle_df[score_cols].values, N, ref_points)\n",
    "        return cycle_df.iloc[selected_indices]\n",
    "    elif filter_type == \"sum\":\n",
    "        return cycle_df.sort_values(\"sum_score\", ascending=True).head(N)\n",
    "    elif filter_type == \"round\":\n",
    "        cycle_df = cycle_df.copy()\n",
    "        for i in range(len(score_cols)):\n",
    "            cycle_df[score_cols[i]+\"_round\"] = cycle_df[score_cols[i]].round(1)\n",
    "        round_cols = [score_cols[i]+\"_round\" for i in range(len(score_cols))]\n",
    "        return cycle_df.sort_values(round_cols, ascending=True).head(N)\n",
    "    elif filter_type == \"non_dominated\":\n",
    "        ranks = nondominated_rank(cycle_df[score_cols].values)\n",
    "        return cycle_df.iloc[np.argsort(ranks)][:N]\n",
    "    elif filter_type == \"cycle\":\n",
    "        return cycle_df[cycle_df[\"cycle\"]==cycle]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown filter type: {filter_type}\")\n",
    "\n",
    "def process_df_by_type(df, CYCLE, filter_type, **kwargs):\n",
    "    dfs = {cycle+1: get_filtered_df(df, cycle, filter_type, **kwargs) \n",
    "           for cycle in range(CYCLE)}\n",
    "    df_merge = pd.concat(dfs)\n",
    "    df_merge.index.names = [\"CYCLE\", \"index\"]\n",
    "    return df_merge.reset_index()\n",
    "\n",
    "# Initialize containers for different filtering methods\n",
    "filter_types = [\"all\", \"top\", \"hv\", \"filter\", \"sum\", \"round\", \"non_dominated\", \"cycle\"]\n",
    "df_merges = {filter_type: [] for filter_type in filter_types}\n",
    "df_merge_cats = {}\n",
    "\n",
    "# Process each dataset\n",
    "for i in range(len(dfs)):\n",
    "    target = configs[i][\"target\"]\n",
    "    exp = configs[i][\"exp\"]\n",
    "    df = dfs[i]\n",
    "    CYCLE = configs[i][\"MAXCYCLE\"]\n",
    "    \n",
    "    # Process with each filter type\n",
    "    for filter_type in filter_types:\n",
    "        kwargs = {\"N\": N}\n",
    "        if filter_type in [\"hv\", \"round\", \"non_dominated\"]:\n",
    "            kwargs.update({\"score_cols\": score_cols, \"ref_points\": ref_points})\n",
    "            \n",
    "        df_merge = process_df_by_type(df, CYCLE, filter_type, **kwargs)\n",
    "        df_merges[filter_type].append(df_merge)\n",
    "\n",
    "# Concatenate results\n",
    "for filter_type in filter_types:\n",
    "    df_merge_cats[filter_type] = pd.concat(df_merges[filter_type])\n",
    "\n",
    "# For backward compatibility\n",
    "all_df_merges = df_merges[\"all\"]\n",
    "top_df_merges = df_merges[\"top\"] \n",
    "hv_df_merges = df_merges[\"hv\"]\n",
    "filter_df_merges = df_merges[\"filter\"]\n",
    "sum_df_merges = df_merges[\"sum\"]\n",
    "round_df_merges = df_merges[\"round\"]\n",
    "non_dominated_df_merges = df_merges[\"non_dominated\"]\n",
    "cycle_df_merges = df_merges[\"cycle\"]\n",
    "all_df_merge_cat = df_merge_cats[\"all\"]\n",
    "top_df_merge_cat = df_merge_cats[\"top\"]\n",
    "hv_df_merge_cat = df_merge_cats[\"hv\"]\n",
    "filter_df_merge_cat = df_merge_cats[\"filter\"]\n",
    "sum_df_merge_cat = df_merge_cats[\"sum\"]\n",
    "round_df_merge_cat = df_merge_cats[\"round\"]\n",
    "non_dominated_df_merge_cat = df_merge_cats[\"non_dominated\"]\n",
    "cycle_df_merge_cat = df_merge_cats[\"cycle\"]\n",
    "X = round_df_merge_cat\n",
    "\n",
    "def calculate_diversity_metrics(df, emb, conf):\n",
    "    \"\"\"Calculate diversity metrics, mutation means and medians for a dataframe\"\"\"\n",
    "    # Calculate diversity for each cycle\n",
    "    divs = {cycle: 1-calculate_mean_similarity(emb[df[(df[\"CYCLE\"]==cycle)][\"index\"].values]) \n",
    "            for cycle in range(1,11)}\n",
    "    \n",
    "    # Calculate mutation statistics\n",
    "    mean_muts = pd.Series({cycle: df[df[\"CYCLE\"]==cycle][\"#Mutation\"].mean() \n",
    "                          for cycle in range(1,11)})\n",
    "    med_muts = pd.Series({cycle: df[df[\"CYCLE\"]==cycle][\"#Mutation\"].median() \n",
    "                         for cycle in range(1,11)})\n",
    "    \n",
    "    # Create output dataframe\n",
    "    div_df = pd.Series(divs)\n",
    "    div_df.index.name = \"CYCLE\"\n",
    "    div_df.name = \"Diversity\"\n",
    "    div_df = div_df.reset_index()\n",
    "    \n",
    "    # Add additional columns\n",
    "    div_df[\"mean_mutation_num\"] = mean_muts.values\n",
    "    div_df[\"median_mutation_num\"] = med_muts.values\n",
    "    div_df[\"target\"] = conf[\"target\"]\n",
    "    div_df[\"model_type\"] = conf[\"model_type\"]\n",
    "    div_df[\"exp\"] = conf[\"exp\"]\n",
    "    \n",
    "    return div_df\n",
    "\n",
    "# Initialize containers for diversity metrics\n",
    "all_divs, top_divs, hv_divs, filter_divs, sum_divs, round_divs, non_dominated_divs, cycle_divs = [], [], [], [], [], [], [], []\n",
    "\n",
    "# Process each dataset\n",
    "for i in range(len(dfs)):\n",
    "    conf = configs[i]\n",
    "    \n",
    "    # Load embedding data\n",
    "    input_dir = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], \n",
    "                            conf[\"exp\"], \"9\", \"train_data\")\n",
    "    emb = np.load(os.path.join(input_dir, \"embedding.npy\"))\n",
    "    \n",
    "    # Calculate diversity metrics for each filtering method\n",
    "    all_divs.append(calculate_diversity_metrics(all_df_merges[i], emb, conf))\n",
    "    top_divs.append(calculate_diversity_metrics(top_df_merges[i], emb, conf))\n",
    "    hv_divs.append(calculate_diversity_metrics(hv_df_merges[i], emb, conf))\n",
    "    filter_divs.append(calculate_diversity_metrics(filter_df_merges[i], emb, conf))\n",
    "    sum_divs.append(calculate_diversity_metrics(sum_df_merges[i], emb, conf))\n",
    "    round_divs.append(calculate_diversity_metrics(round_df_merges[i], emb, conf))\n",
    "    non_dominated_divs.append(calculate_diversity_metrics(non_dominated_df_merges[i], emb, conf))\n",
    "    cycle_divs.append(calculate_diversity_metrics(cycle_df_merges[i], emb, conf))\n",
    "\n",
    "# Concatenate results\n",
    "all_divs_cat = pd.concat(all_divs, ignore_index=True)\n",
    "top_divs_cat = pd.concat(top_divs, ignore_index=True)\n",
    "hv_divs_cat = pd.concat(hv_divs, ignore_index=True)\n",
    "filter_divs_cat = pd.concat(filter_divs, ignore_index=True)\n",
    "sum_divs_cat = pd.concat(sum_divs, ignore_index=True)\n",
    "round_divs_cat = pd.concat(round_divs, ignore_index=True)\n",
    "non_dominated_divs_cat = pd.concat(non_dominated_divs, ignore_index=True)\n",
    "cycle_divs_cat = pd.concat(cycle_divs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dd8c5-8eec-4972-a21e-970c56b2d248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configdf = pd.DataFrame(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66214974-80c7-4dc7-9f28-f26f511080e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_scores=[]\n",
    "for conf in configs:\n",
    "    target = conf[\"target\"]\n",
    "    # if target not in targets:\n",
    "    #     continue\n",
    "    for cycle in range(10):\n",
    "        input_dir = os.path.join(data_dir, conf[\"target\"], conf[\"model_type\"], conf[\"exp\"], str(cycle), \"train_data\")\n",
    "        test_pred = np.load(os.path.join(input_dir, \"test_inference_bias.npy\"))\n",
    "        test_df = test_dfs[target]\n",
    "        test_df_ = test_df.copy()\n",
    "        test_df_[\"Pred\"] = test_pred\n",
    "        all_test_scores.append({\n",
    "            **calc_test(test_df_[\"DMS_score\"], test_df_[\"Pred\"]),\n",
    "            \"CYCLE\": cycle+1,\n",
    "            \"target\": conf[\"target\"],\n",
    "            \"model_type\": conf[\"model_type\"],\n",
    "            \"exp\": conf[\"exp\"],\n",
    "        })\n",
    "all_test_scores_cat = pd.DataFrame(all_test_scores)\n",
    "all_test_scores_cat[\"spearman\"] = all_test_scores_cat[\"spearman\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b18b4-ea22-4c3e-8af7-0e0e46792c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df_merge_cat.to_csv(\"results/flexddg_offline/multi/sum_results.csv\",index=False)\n",
    "all_test_scores_cat.to_csv(\"results/flexddg_offline/multi/all_results_test.csv\",index=False)\n",
    "all_df_merge_cat.to_csv(\"results/flexddg_offline/multi/all_results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg39",
   "language": "python",
   "name": "pyg39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
